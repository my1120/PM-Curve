---
title: "df selection and model assessment"
author: "Meilin Yan"
date: "September 27, 2016"
output: html_document
---
\
Use all-cause mortality for the following analysis.

### Packages Loading
```{r message=FALSE, warning=FALSE}
library(dlnm)
library(splines)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(knitr)
```

### Data prep
```{r}
bj <- read.csv("0812bj.csv")
bj$date <- as.Date(bj$date)
bj$year <- year(bj$date)
bj$month <- month(bj$date)
bj$month <- as.factor(bj$month)
bj$tot <- bj$A00toR99
bj$cir <- bj$I00toI99
bj$resp <- bj$J00toJ99
bj <- subset(bj, select = c("date", "year", "month", "tmean", "rh", "pm25hdbl",
                            "pm25ussg", "o3", "time", "dow", "tot", "cir", "resp"))
```

#### Holidays
Set up holiday variable.

```{r echo=FALSE}
holi <- read.csv("beijing_date_holiday.csv")
holi$date <- as.Date(holi$date)
holiday1 <- holi$date[holi$Holiday == 1]

# Create holiday for dates not included in holi data
# summary(holi$date)
# 2008
a1 <- as.Date("2008-01-01")
a2 <- seq(as.Date("2008-02-06"), as.Date("2008-02-12"), by=1)
a3 <- seq(as.Date("2008-04-04"), as.Date("2008-04-06"), by=1)
a4 <- seq(as.Date("2008-05-01"), as.Date("2008-05-03"), by=1)
a5 <- seq(as.Date("2008-06-07"), as.Date("2008-06-09"), by=1)
a6 <- seq(as.Date("2008-09-13"), as.Date("2008-09-15"), by=1)
a7 <- seq(as.Date("2008-09-29"), as.Date("2008-10-05"), by=1)

# 2009
b1 <- seq(as.Date("2009-01-01"), as.Date("2009-01-03"), by=1)
b2 <- seq(as.Date("2009-01-25"), as.Date("2009-01-31"), by=1)

holiday <- unique(c(holiday1,a1,a2,a3,a4,a5,a6,a7,b1,b2))
bj$holiday <- ifelse(bj$date %in% holiday, 1, 0)
bj$holiday <- factor(bj$holiday, levels = c(1, 0))
```

#### Exclude days of 2008
```{r}
bj <- subset(bj, year != 2008)
bj$year <- as.factor(bj$year)
```

#### Prediction and imputation
```{r echo=FALSE}
# Observed PM from Beijing (Non-embassy) monitor
bj$pm_bj <- bj$pm25hdbl

# Observed PM from Embassy monitor
bj$pm_us <- bj$pm25ussg

# predict pm_bj with pm_us
mod_bj <- lm(pm_bj ~ pm_us, na.action = na.exclude, data = bj)
bj$pre_bj <- predict(mod_bj, newdata = bj)
bj$pm_bj <- ifelse(is.na(bj$pm_bj), bj$pre_bj, bj$pm_bj)
# After replacing NA with predicted values, it has only 5 NA.
# The observed data from non-Embassy monitor has 142 NA.

# predict pm_us with pm_bj
mod_us <- lm(pm_us ~ pm_bj, data = bj)
bj$pre_us <- predict(mod_us, newdata = bj)
bj$pm_us <- ifelse(is.na(bj$pm_us), bj$pre_us, bj$pm_us)
# After replacing NA with predicted values, it has only 5 NA.
# The observed data from US Embassy has 100 NA.

# Get the average value of PM2.5 
bj$ave_pm <- (bj$pm_bj + bj$pm_us)/2

# Create lag01 PM2.5 
bj$bj_pm01 <- filter(bj$pm25hdbl, c(1,1)/2, sides = 1)
bj$bj_pm01 <- as.numeric(bj$bj_pm01)
# bj$bj_pm01 <- round(bj$bj_pm01, 2)

bj$us_pm01 <- filter(bj$pm25ussg, c(1,1)/2, sides = 1)
bj$us_pm01 <- as.numeric(bj$us_pm01)
# bj$us_pm01 <- round(bj$us_pm01, 2)

bj$pm01 <- filter(bj$ave_pm, c(1,1)/2, sides = 1)
bj$pm01 <- as.numeric(bj$pm01)
# summary(bj$pm01)
# 10 NA
```

**ave_pm is the avearge PM levels, pm01 is the lag01 PM levels.**

#### Subset cold season and warm season
```{r}
bj.warm <- subset(bj, quarters(date) %in% c("Q2", "Q3"))
bj.cold <- subset(bj, quarters(date) %in% c("Q1", "Q4"))
```

```{r echo=FALSE}
# Generate "group" for bj.cold
bj.cold$gr[bj.cold$year == 2009 & bj.cold$month %in% c(1:3)] <- "g1"

bj.cold$gr[bj.cold$year == 2009 & bj.cold$month %in% c(10:12)] <- "g2"
bj.cold$gr[bj.cold$year == 2010 & bj.cold$month %in% c(1:3)] <- "g2"

bj.cold$gr[bj.cold$year == 2010 & bj.cold$month %in% c(10:12)] <- "g3"
bj.cold$gr[bj.cold$year == 2011 & bj.cold$month %in% c(1:3)] <- "g3"

bj.cold$gr[bj.cold$year == 2011 & bj.cold$month %in% c(10:12)] <- "g4"
bj.cold$gr[bj.cold$year == 2012 & bj.cold$month %in% c(1:3)] <- "g4"

bj.cold$gr[bj.cold$year == 2012 & bj.cold$month %in% c(10:12)] <- "g5"
bj.cold$gr <- as.factor(bj.cold$gr)
```

### df selection
#### data-driven method: choose df that minimizes AIC/BIC
**Functions**
```{r message=FALSE, warning=FALSE}
# GLM model
fit <- function(data = c(), df=c()) {
  mod <- glm(pm01 ~ ns(tmean,3) + ns(rh, 3) + ns(time, 4*df) +
                 dow + holiday,
               data = data,
               na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))
  
  aic <- round(AIC(mod), 2)
  bic <- round(BIC(mod), 2)
  dev <- summary(mod)$deviance
  
  out <- c(model = mod, aic = aic, bic = bic, dev = dev)
  return(out)
}
```

```{r message=FALSE, warning=FALSE}
Df <- seq(4, 15, 1)
df_tab_1 <- data.frame(df = Df, AIC = NA, BIC = NA, Deviance = NA)
for(i in 1:length(Df)){
  fun <- fit(data = bj, df = Df[i])
  df_tab_1[i, "AIC"] <- fun$aic
  df_tab_1[i, "BIC"] <- fun$bic
  df_tab_1[i, "Deviance"] <- fun$dev
}

kable(df_tab_1, align = "c")

```

\
I would think 6, 7 or 8 df per year might be reasonable based on these criteria and previous knowledge.
\
From Roger's book, methods based on predicting the exposure variable include generialized cross-validation and AIC, while BIC is used over a set of models constructed to predict the outcome. It doesn't hurt to show both AIC and BIC here, but we need to think about what to show in the manuscript.

### Model assessment (linear vs. non-linear)

#### Use k-fold cross-validation
**Use 6 df per year**
```{r message=FALSE, warning=FALSE}
library(caret)
tc <- trainControl(method = "cv", number = 10)

Lmod <- train(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 2 df for PM
Smod2 <- train(tot ~ ns(pm01, df = 2) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 3 df for PM
Smod3 <- train(tot ~ ns(pm01, df = 3) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 4 df for PM
Smod4 <- train(tot ~ ns(pm01, df = 4) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)

com.1 <- data.frame(model = c("Linear", "Non-lin 2 df", "Non-lin 3 df", "Non-lin 4 df"), 
                    RMSE = NA)
com.1[, 2] <- c(Lmod$results[1, 2], Smod2$results[1, 2], 
                Smod3$results[1, 2], Smod4$results[1, 2])
kable(com.1, format = "markdown", align = "c")
```

\
__RMSE: root mean squared error__

\
**Use 7 df per year**
```{r echo=FALSE}
tc <- trainControl(method = "cv", number = 10)

Lmod <- train(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 2 df for PM
Smod2 <- train(tot ~ ns(pm01, df = 2) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 3 df for PM
Smod3 <- train(tot ~ ns(pm01, df = 3) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 4 df for PM
Smod4 <- train(tot ~ ns(pm01, df = 4) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)

com.2 <- data.frame(model = c("Linear", "Non-lin 2 df", "Non-lin 3 df", "Non-lin 4 df"), 
                    RMSE = NA)
com.2[, 2] <- c(Lmod$results[1, 2], Smod2$results[1, 2], 
                Smod3$results[1, 2], Smod4$results[1, 2])
kable(com.2, format = "markdown", align = "c")
```

\
__1.Linear and non-linear models perform almost the same.__
\
__2.Now we have 3 df for PM, which is reasonable, I think.__

##### Confirm by coding k-fold cross-validation
```{r}
#Randomly shuffle the data
set.seed(1)
Data <- bj
Data <- Data[sample(nrow(Data)), ]

#Create 10 equally size folds
folds <- cut(seq(1, nrow(Data)), breaks=10, labels=FALSE)

prediction.l <- data.frame()
prediction.s2 <- data.frame()
prediction.s3 <- data.frame()
prediction.s4 <- data.frame()

testDataCopy <- data.frame()

#Perform 10 fold cross validation
for(i in 1:10){
  #Segement your data by fold using the which() function 
  testIndexes <- which(folds == i, arr.ind = TRUE)
  
  testData <- Data[testIndexes, ]
  trainData <- Data[-testIndexes, ]

  fit.l <- glm(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  # 2 df for pm
  fit.s2 <- glm(tot ~ ns(pm01, 2) + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  # 3 df for pm
  fit.s3 <- glm(tot ~ ns(pm01, 3) + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  # 4 df for pm
  fit.s4 <- glm(tot ~ ns(pm01, 4) + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  
  pred.l <- as.data.frame(predict(fit.l, newdata = testData, type = "response"))
  prediction.l <- rbind(prediction.l, pred.l)
  
  pre.s2 <- as.data.frame(predict(fit.s2, newdata = testData, type = "response"))
  prediction.s2 <- rbind(prediction.s2, pre.s2)
  
  pre.s3 <- as.data.frame(predict(fit.s3, newdata = testData, type = "response"))
  prediction.s3 <- rbind(prediction.s3, pre.s3)

  pre.s4 <- as.data.frame(predict(fit.s4, newdata = testData, type = "response"))
  prediction.s4 <- rbind(prediction.s4, pre.s4)
  
  testDataCopy <- rbind(testDataCopy, as.data.frame(testData[, "tot"]))
}

result <- cbind(prediction.l, prediction.s2, prediction.s3, prediction.s4, testDataCopy)
names(result) <- c("Predicted.l", "Predicted.s2", "Predicted.s3", "Predicted.s4", "Actual")
head(result) # Take a look of it

com.3 <- data.frame(model = c("Linear", "Non-lin 2 df", "Non-lin 3 df", "Non-lin 4 df"), 
                    MSE = NA)

com.3[1, 2] <- mean((result[, 5] - result[, 1])^2, na.rm = TRUE)
com.3[2, 2] <- mean((result[, 5] - result[, 2])^2, na.rm = TRUE)
com.3[3, 2] <- mean((result[, 5] - result[, 3])^2, na.rm = TRUE)
com.3[4, 2] <- mean((result[, 5] - result[, 4])^2, na.rm = TRUE)
com.3$RMSE <- sqrt(com.3$MSE)
kable(com.3, format = "markdown", align = "c")
```


#### Use QAIC
```{r message=FALSE, warning=FALSE}
# QAIC function
fqaic <- function(model = c()) {
  loglik <- sum(dpois(model$y, model$fitted.values, log=TRUE))
  phi <- summary(model)$dispersion
  qaic <- -2*loglik + 2*summary(model)$df[3]*phi
  return(qaic)
}

lin.mod <- glm(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) +
                 dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj, na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

spl.mod <- glm(tot ~ ns(pm01, knots = c(75, 150)) + ns(tmean,3) + ns(rh, 3) + 
                          ns(time, 4*6) + dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj, na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

c(fqaic(lin.mod), fqaic(spl.mod))
```
\
__Samle conclusion, linear ane non-linear perform pretty similar.__

### Subset method by classifing PM concentration

```{r cache=TRUE}
# re-order by pm
bj.sub <- bj[order(bj$pm01), ]

# Create 4 equally size categories
bj.sub$pm.cate <- cut(seq(1, nrow(bj.sub)), breaks=4, labels=FALSE)
bj.sub$pm.cate <- as.factor(bj.sub$pm.cate)

ori.fit <- glm(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) +
                 dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj.sub,
               na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

# fit with pm.cate, ~ pm01 + pm.cate + pm01:pm.cate
cat.fit <- glm(tot ~ pm01*pm.cate + ns(tmean,3) + ns(rh, 3) + 
                 ns(time, 4*6) + dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj.sub,
               na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

anova(ori.fit, cat.fit, test = "F")
```
\
Not significant, suggesting subset model might not be better than original model. Is that right?

#### Extract and plot effects of PM in each catetory

__I am wondering whether we could get the effect of PM in each PM category from the fitted values. But I have no idea how to calculate__
```{r eval=FALSE}
fitted.death <- round(predict(cat.fit, newdata = bj.sub, type = "response"), 0)
actual.death <- bj.sub$tot
pm.category <- bj.sub$pm.cate
  
dat <- data.frame(cbind(actual.death, fitted.death, pm.category))
```

__Also tried to get from the summary of model, but seems an odd results. Not sure if it's correct__
```{r}
sum.coef <- summary(cat.fit)$coefficients
sum.coef[c(2:5, 43:45), ]

sub.tab <- data.frame(pm_category = c("C1", "C2", "C3", "C4"), Coef = NA)
sub.tab[1, 2] <- sum.coef[2, 1]
sub.tab[2, 2] <- sum.coef[2, 1] + sum.coef[43, 1]
sub.tab[3, 2] <- sum.coef[2, 1] + sum.coef[44, 1]
sub.tab[4, 2] <- sum.coef[2, 1] + sum.coef[45, 1]
kable(sub.tab, align = "c")
```


