---
title: "df selection, model assessment, and subset method"
author: "Meilin Yan"
date: "September 27, 2016"
output: word_document
---

Use all-cause mortality for the following analysis.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dlnm)
library(splines)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(knitr)
library(caret)

bj <- read.csv("0812bj.csv")
bj$date <- as.Date(bj$date)
bj$year <- year(bj$date)
bj$month <- month(bj$date)
bj$month <- as.factor(bj$month)
bj$tot <- bj$A00toR99
bj$cir <- bj$I00toI99
bj$resp <- bj$J00toJ99
bj <- subset(bj, select = c("date", "year", "month", "tmean", "rh", "pm25hdbl",
                            "pm25ussg", "o3", "time", "dow", "tot", "cir", "resp"))
# set up holiday variable
holi <- read.csv("beijing_date_holiday.csv")
holi$date <- as.Date(holi$date)
holiday1 <- holi$date[holi$Holiday == 1]

# Create holiday for dates not included in holi data
# summary(holi$date)
# 2008
a1 <- as.Date("2008-01-01")
a2 <- seq(as.Date("2008-02-06"), as.Date("2008-02-12"), by=1)
a3 <- seq(as.Date("2008-04-04"), as.Date("2008-04-06"), by=1)
a4 <- seq(as.Date("2008-05-01"), as.Date("2008-05-03"), by=1)
a5 <- seq(as.Date("2008-06-07"), as.Date("2008-06-09"), by=1)
a6 <- seq(as.Date("2008-09-13"), as.Date("2008-09-15"), by=1)
a7 <- seq(as.Date("2008-09-29"), as.Date("2008-10-05"), by=1)

# 2009
b1 <- seq(as.Date("2009-01-01"), as.Date("2009-01-03"), by=1)
b2 <- seq(as.Date("2009-01-25"), as.Date("2009-01-31"), by=1)

holiday <- unique(c(holiday1,a1,a2,a3,a4,a5,a6,a7,b1,b2))
bj$holiday <- ifelse(bj$date %in% holiday, 1, 0)
bj$holiday <- factor(bj$holiday, levels = c(1, 0))

# Exclude 2008
bj <- subset(bj, year != 2008)
bj$year <- as.factor(bj$year)

### Prediction and imputation 
# Observed PM from Beijing (Non-embassy) monitor
bj$pm_bj <- bj$pm25hdbl

# Observed PM from Embassy monitor
bj$pm_us <- bj$pm25ussg

# predict pm_bj with pm_us
mod_bj <- lm(pm_bj ~ pm_us, na.action = na.exclude, data = bj)
bj$pre_bj <- predict(mod_bj, newdata = bj)
bj$pm_bj <- ifelse(is.na(bj$pm_bj), bj$pre_bj, bj$pm_bj)
# After replacing NA with predicted values, it has only 5 NA.
# The observed data from non-Embassy monitor has 142 NA.

# predict pm_us with pm_bj
mod_us <- lm(pm_us ~ pm_bj, data = bj)
bj$pre_us <- predict(mod_us, newdata = bj)
bj$pm_us <- ifelse(is.na(bj$pm_us), bj$pre_us, bj$pm_us)
# After replacing NA with predicted values, it has only 5 NA.
# The observed data from US Embassy has 100 NA.

# Get the average value of PM2.5 
bj$ave_pm <- (bj$pm_bj + bj$pm_us)/2

# Create lag01 PM2.5 
bj$bj_pm01 <- filter(bj$pm25hdbl, c(1,1)/2, sides = 1)
bj$bj_pm01 <- as.numeric(bj$bj_pm01)
# bj$bj_pm01 <- round(bj$bj_pm01, 2)

bj$us_pm01 <- filter(bj$pm25ussg, c(1,1)/2, sides = 1)
bj$us_pm01 <- as.numeric(bj$us_pm01)
# bj$us_pm01 <- round(bj$us_pm01, 2)

bj$pm01 <- filter(bj$ave_pm, c(1,1)/2, sides = 1)
bj$pm01 <- as.numeric(bj$pm01)
# summary(bj$pm01)
# 10 NA
```

ave_pm is the avearge PM levels, pm01 is the lag01 PM levels according to ave_pm.

```{r eval=FALSE, echo=FALSE}
# seasons
bj.warm <- subset(bj, quarters(date) %in% c("Q2", "Q3"))
bj.cold <- subset(bj, quarters(date) %in% c("Q1", "Q4"))

# Generate "group" for bj.cold
bj.cold$gr[bj.cold$year == 2009 & bj.cold$month %in% c(1:3)] <- "g1"

bj.cold$gr[bj.cold$year == 2009 & bj.cold$month %in% c(10:12)] <- "g2"
bj.cold$gr[bj.cold$year == 2010 & bj.cold$month %in% c(1:3)] <- "g2"

bj.cold$gr[bj.cold$year == 2010 & bj.cold$month %in% c(10:12)] <- "g3"
bj.cold$gr[bj.cold$year == 2011 & bj.cold$month %in% c(1:3)] <- "g3"

bj.cold$gr[bj.cold$year == 2011 & bj.cold$month %in% c(10:12)] <- "g4"
bj.cold$gr[bj.cold$year == 2012 & bj.cold$month %in% c(1:3)] <- "g4"

bj.cold$gr[bj.cold$year == 2012 & bj.cold$month %in% c(10:12)] <- "g5"
bj.cold$gr <- as.factor(bj.cold$gr)
```

## df selection

#### Data-driven method: choose df that minimizes AIC/BIC/deviance
We used a smooth function of time to control for unmesured confounders. We selected the degree of smoothness by minimizing a goodness-of-fit criterion. In this study, we are interested in estimating log-relative risk of $PM_{2.5}$ on death outcome, rather than fitting a model which could best predict the death outcome. So our methods should be based on predicting the $PM_{2.5}$ exposure concentration.(Ref: "Statistical Methods for Environmental Epidemiology with R")  

A criterion is minimized over a set of models constructed to predict the $PM_{2.5}$ exposure concentreion with different degree of smoothness of time. Criteria used in this study are AIC, BIC, and deviance.  

[MY: Actually BIC is a criterion for models predicting outcome according to Roger's book (Page 89-90.]  


```{r message=FALSE, warning=FALSE, cache=TRUE}
# GLM model
fit <- function(data = c(), df=c()) {
  mod <- glm(pm01 ~ ns(tmean,3) + ns(rh, 3) + ns(time, 4*df) +
                 dow + holiday,
               data = data,
               na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))
  
  aic <- round(AIC(mod), 2)
  bic <- round(BIC(mod), 2)
  dev <- summary(mod)$deviance
  
  out <- c(mod = mod, aic = aic, bic = bic, dev = dev)
  return(out)
}

Df <- seq(4, 15, 1)
df_tab_1 <- data.frame(df = Df, AIC = NA, BIC = NA, Deviance = NA)
for(i in 1:length(Df)){
  fun <- fit(data = bj, df = Df[i])
  df_tab_1[i, "AIC"] <- fun$aic
  df_tab_1[i, "BIC"] <- fun$bic
  df_tab_1[i, "Deviance"] <- fun$dev
}

kable(df_tab_1, align = "c")

```

  
  
In consideration of both the results of criterion-based procedures and previous knowledge of investigating association between moratliy and air pollution, I would choose 7 degree of smoothness of time.  


\
**Why is BIC optimizing for a lower df than AIC?**  
As shown in the table above, AIC is minimized by selecting 8 df of time, while BIC is minimized by a choice of 5 df of time. Because BIC penalizes heavily on larger model, so it will tend to prefer simpler model. The equation of Akaike Information Criterion (AIC) and Bayes Information Criterion (BIC) are as follows,
$$AIC = n*log(RSS/n) + 2p$$
$$BIC = n*log(RSS/n) + log(n)p$$
log(n) > 2 for any n > 7, so BIC generally places a heavier penality on larger model and prefers simpler model.  


\
**RSS vs. Deviance for a linear model. Why is observed deviance trend not surprising?**  
We choose degree of smootheness of time by fitting a linear model predicting the exposure $PM_{2.5}$ concentration. So residual sum of squares (RSS) is equivalent to the deviance because Oridnary Least Squares (OLS) and Maximum Likelihood (ML) estimates conincide in this case.  

Deviance is the comparison of the fitted model and the saturated model, which assumes that each observation has its own parameter (i.e. n parameters).
$$Residual\ Deviance = 2(loglikliehood(saturated\ model) - loglikelihood(fitted\ model))$$
As I increase the degree of smoothness of time, the number of parameters is also increased, which means the fitted model here is getting closer to the saturated model for the same data set. Therefore, the deviance is decrease as model flexibility increase.  


\
**How do AIC and BIC measures differ from cross-validation measures?**  
AIC and BIC measures are based on the training data, so the RSS/MSE is calculated for the training data, which is the data we used to fit model. However, cross-validation measure minimizes the test MSE based on testing data set which is not used to train the statistical learning method. So the method/model with lowest training MSE may not also have the lowest test MSE.  

## Model assessment (linear vs. non-linear)

#### Use k-fold cross-validation  
**Use 7 df per year**  
```{r cache=TRUE}
tc <- trainControl(method = "cv", number = 10)

Lmod <- train(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 2 df for PM
Smod2 <- train(tot ~ ns(pm01, df = 2) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 3 df for PM
Smod3 <- train(tot ~ ns(pm01, df = 3) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)
# 4 df for PM
Smod4 <- train(tot ~ ns(pm01, df = 4) + ns(tmean,3) + ns(rh, 3) + ns(time, 4*7) + dow + holiday, 
                 data = bj, trControl = tc, method = "glm", 
                 family = quasipoisson(link = "log"),
                 na.action = na.exclude)

com.2 <- data.frame(model = c("Linear", "Non-lin 2 df", "Non-lin 3 df", "Non-lin 4 df"),
                    RMSE = NA)
com.2[, 2] <- c(Lmod$results[1, 2], Smod2$results[1, 2], 
                Smod3$results[1, 2], Smod4$results[1, 2])

kable(com.2, format = "markdown", align = "c")
# We concluded that the linear model and the non-linear (spline) model perform almost the same.

```
    
  
**Why don't we expect the values of RMSE be exactly the same even though they are pretty consistent?**  
The test MSE of a given observation in the testing data can be decomposed into the sum of three elements: the variance of $\hat y_i$, the squared bias of $\hat y_i$ and the variance of the error term $\epsilon$. (Ref: An Introduction to Statistical Learning.)  

$$E(y_i - \hat y_i)^2 = Var(\hat y_i) + (Bias(\hat y_i))^2 + Var(\epsilon)$$

The overall test MSE can be calculated by averaging $E(y_i - \hat y_i)^2$ over all the observations in the testing data set.  

In general, more flexiable statistical methods have higher variance of $\hat y_i$, and lower bias of $\hat y_i$. So among the four models, the one with a linear term of $PM_{2.5}$ has the lowest flexibility, while the model with a 4-df spline term of $PM_{2.5}$ has the highest flexibility. However, whether the test MSE increases or decreases depends on the relative rate of the change of variance of $\hat y_i$ and the bias of $\hat y_i$. Therefore, the RMSE (root of MSE) values are slightly different for these models.

```{r eval=FALSE, echo=FALSE}
#Randomly shuffle the data
set.seed(1)
Data <- bj
Data <- Data[sample(nrow(Data)), ]

#Create 10 equally size folds
folds <- cut(seq(1, nrow(Data)), breaks=10, labels=FALSE)

prediction.l <- data.frame()
prediction.s2 <- data.frame()
prediction.s3 <- data.frame()
prediction.s4 <- data.frame()

testDataCopy <- data.frame()

#Perform 10 fold cross validation
for(i in 1:10){
  #Segement your data by fold using the which() function 
  testIndexes <- which(folds == i, arr.ind = TRUE)
  
  testData <- Data[testIndexes, ]
  trainData <- Data[-testIndexes, ]

  fit.l <- glm(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  # 2 df for pm
  fit.s2 <- glm(tot ~ ns(pm01, 2) + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  # 3 df for pm
  fit.s3 <- glm(tot ~ ns(pm01, 3) + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  # 4 df for pm
  fit.s4 <- glm(tot ~ ns(pm01, 4) + ns(tmean,3) + ns(rh, 3) +  ns(time, 4*6) + dow + holiday,
           family = quasipoisson(link = "log"),
           data = trainData, na.action = na.exclude,
           control = glm.control(epsilon = 10E-8, maxit = 10000))
  
  pred.l <- as.data.frame(predict(fit.l, newdata = testData, type = "response"))
  prediction.l <- rbind(prediction.l, pred.l)
  
  pre.s2 <- as.data.frame(predict(fit.s2, newdata = testData, type = "response"))
  prediction.s2 <- rbind(prediction.s2, pre.s2)
  
  pre.s3 <- as.data.frame(predict(fit.s3, newdata = testData, type = "response"))
  prediction.s3 <- rbind(prediction.s3, pre.s3)

  pre.s4 <- as.data.frame(predict(fit.s4, newdata = testData, type = "response"))
  prediction.s4 <- rbind(prediction.s4, pre.s4)
  
  testDataCopy <- rbind(testDataCopy, as.data.frame(testData[, "tot"]))
}

result <- cbind(prediction.l, prediction.s2, prediction.s3, prediction.s4, testDataCopy)
names(result) <- c("Predicted.l", "Predicted.s2", "Predicted.s3", "Predicted.s4", "Actual")
head(result) # Take a look of it

com.3 <- data.frame(model = c("Linear", "Non-lin 2 df", "Non-lin 3 df", "Non-lin 4 df"), 
                    MSE = NA)

com.3[1, 2] <- mean((result[, 5] - result[, 1])^2, na.rm = TRUE)
com.3[2, 2] <- mean((result[, 5] - result[, 2])^2, na.rm = TRUE)
com.3[3, 2] <- mean((result[, 5] - result[, 3])^2, na.rm = TRUE)
com.3[4, 2] <- mean((result[, 5] - result[, 4])^2, na.rm = TRUE)
com.3$RMSE <- sqrt(com.3$MSE)
kable(com.3, format = "markdown", align = "c")
```


#### Use QAIC
```{r message=FALSE, warning=FALSE, cache = TRUE}
# QAIC function
fqaic <- function(model = c()) {
  loglik <- sum(dpois(model$y, model$fitted.values, log=TRUE))
  phi <- summary(model)$dispersion
  qaic <- -2*loglik + 2*summary(model)$df[3]*phi
  return(qaic)
}

lin.mod <- glm(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) +
                 dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj, na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

spl.mod <- glm(tot ~ ns(pm01, knots = c(75, 150)) + ns(tmean,3) + ns(rh, 3) + 
                          ns(time, 4*6) + dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj, na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

c(fqaic(lin.mod), fqaic(spl.mod))

# The QAIC values are very close for linear and non-linear model, so these two models fitted quite similar for our data.

```  
  
  
## Subset method by classifing PM concentration
**Compare the original model with the subset model**
```{r cache=TRUE}
# Create 4 equally size categories
bj$pm.cate <- cut(bj$pm01, breaks = c(quantile(bj$pm01, 
                                             probs = seq(0, 1, by = 0.25), 
                                             na.rm = TRUE)), 
                  labels = c("Cagetory 1", "Category 2", "Category 3", "Category 4"),
                  include.lowest = TRUE)

ori.fit <- glm(tot ~ pm01 + ns(tmean,3) + ns(rh, 3) + ns(time, 4*6) +
                 dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj,
               na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

# fit with pm.cate, ~ pm01 + pm.cate + pm01:pm.cate
cat.fit <- glm(tot ~ pm01*pm.cate + ns(tmean,3) + ns(rh, 3) + 
                 ns(time, 4*6) + dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj,
               na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

anova(ori.fit, cat.fit, test = "F")
```

By using a ANOVA test, I am testing whether the subset model which has an interaction of pm01 and pm.cate performs better than the original model without an interaction term.  
\
$H_0$: *The original model is adequate.*  

$H_a$: *The original model is not adequate.*  

From the ANOVA table, we got an p-value which is not very small, so we concluded that the original model and the subset model are not significantly different with each other.  

**Plot**  

I tried plotting some figures because I am not sure which one is more appropriate.  

```{r cache=TRUE, warning=FALSE, message=FALSE, fig.width=7, fig.height=4}
#### Plot log(Y) vs. PM
## Fit the subset model without intercept
cate.fit <- glm(tot ~ -1 + pm01*pm.cate + ns(tmean,3) + ns(rh, 3) + 
                 ns(time, 4*6) + dow + holiday,
               family = quasipoisson(link = "log"),
               data = bj,
               na.action = na.exclude,
               control = glm.control(epsilon = 10E-8, maxit = 10000))

bj$fitted.log <- predict(cate.fit, newdata = bj) 
# The default of predict is on the scale of the linear predictor

# The first plot has unequal width for each PM category
ggplot(bj, aes(x = pm01, y = fitted.log, colour = pm.cate)) +
  geom_point(alpha = 0.5, size = 1) +
  #geom_abline(slope = -9.614375e-05, intercept = 5.412439)
  geom_smooth(method = "lm") +
  labs(x = expression(PM[2.5] ~ Concentration ~ mu ~ g ~ m^{-3}),
       y = expression(Log ~ (Total ~ Death))) + 
  theme_few() +
  theme(legend.position = "none") 

# The second has equal width for each PM category
# facet_wrap is based on the level of pm.cate, so I omitted the NA. Otherwise it has five levels.
ggplot(na.omit(bj), aes(x = pm01, y = fitted.log)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm") +
  facet_wrap(~ pm.cate, nrow = 1, scales = "free_x") +
  labs(x = expression(PM[2.5] ~ Concentration ~ mu ~ g ~ m^{-3}),
       y = expression(Log ~ (Total ~ Death))) +
  theme_few()

####Plot percent increase per 10 units increase of PM vs. PM category

coefs <- as.data.frame(summary(cate.fit)$coefficients)
coefs[c("pm01", "pm01:pm.cateCategory 2", "pm01:pm.cateCategory 3",
        "pm01:pm.cateCategory 4"), ]

tab <- data.frame(pm_category = c("C1", "C2", "C3", "C4"), Coef = NA, SE = NA)
tab[1, 2] <- coefs["pm01", "Estimate"]
tab[2, 2] <- coefs["pm01", "Estimate"] + coefs["pm01:pm.cateCategory 2", "Estimate"]
tab[3, 2] <- coefs["pm01", "Estimate"] + coefs["pm01:pm.cateCategory 3", "Estimate"]
tab[4, 2] <- coefs["pm01", "Estimate"] + coefs["pm01:pm.cateCategory 4", "Estimate"]

# Actually I am not sure whether it's the right way of calculating SE.
tab[1, 3] <- coefs["pm01", "Std. Error"]
tab[2, 3] <- coefs["pm01", "Std. Error"] + coefs["pm01:pm.cateCategory 2", "Std. Error"]
tab[3, 3] <- coefs["pm01", "Std. Error"] + coefs["pm01:pm.cateCategory 3", "Std. Error"]
tab[4, 3] <- coefs["pm01", "Std. Error"] + coefs["pm01:pm.cateCategory 4", "Std. Error"]

tab$percent <- round((exp(tab$Coef*10) - 1)*100, 2)
tab$low <- round((exp((tab$Coef - 1.96 * tab$SE)*10) - 1)*100, 2)
tab$high <- round((exp((tab$Coef + 1.96 * tab$SE)*10) - 1)*100, 2)
kable(tab)

ggplot(data = tab, aes(x = pm_category, y = percent)) +
  geom_pointrange(aes(ymin = low, ymax = high), size = 0.2) +
  geom_hline(yintercept = 0, color = "grey") +
  labs(x = expression(PM[2.5] ~~ Catetory),
       y = expression("Percent Increase of Death per 10" ~ mu ~ g ~ m^{-3} ~ 
                        "increase of" ~ PM[2.5])) +
  theme(axis.title = element_text(size = 6)) +
  theme_few()
```


